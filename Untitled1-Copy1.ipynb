{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "476ddd06-7dc4-4a35-bf3a-7fd9f9001118",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import logging\n",
    "from composer_one import Composer  # Ensure this file is in the same directory or adjust import\n",
    "from midi2seq import piano2seq, seq2piano  # Ensure these are available\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def process_midi_files(file_paths, seq_length):\n",
    "    sequences = []\n",
    "    for file_path in file_paths:\n",
    "        try:\n",
    "            seq = piano2seq(file_path)\n",
    "            for i in range(0, len(seq) - seq_length, seq_length // 2):\n",
    "                sequences.append(seq[i:i+seq_length])\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing file {file_path}: {str(e)}\")\n",
    "    \n",
    "    if not sequences:\n",
    "        raise ValueError(\"No valid sequences were extracted from the MIDI files.\")\n",
    "    \n",
    "    # Convert list of sequences to a single numpy array\n",
    "    sequences_array = np.array(sequences)\n",
    "    \n",
    "    # Convert numpy array to PyTorch tensor\n",
    "    return torch.from_numpy(sequences_array).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d96c4b38-2e9d-4fa1-bb62-5ecce17e0309",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Found 1184 MIDI files.\n",
      "INFO:Processed 119520 sequences from MIDI files.\n"
     ]
    }
   ],
   "source": [
    "# Define the directory where your MIDI files are stored\n",
    "midi_dir = '/Users/matiwosbirbo/PianoGen/maestro-v1.0.0 2/'\n",
    "midi_files = glob.glob(os.path.join(midi_dir, '*.midi'))\n",
    "\n",
    "if not midi_files:\n",
    "    midi_files = glob.glob(os.path.join(midi_dir, '*.mid'))  # Try .mid extension\n",
    "\n",
    "if not midi_files:\n",
    "    raise FileNotFoundError(f\"No MIDI files found in directory: {midi_dir}\")\n",
    "\n",
    "logging.info(f\"Found {len(midi_files)} MIDI files.\")\n",
    "\n",
    "# Define sequence length for training\n",
    "seq_length = 512\n",
    "\n",
    "try:\n",
    "    training_data = process_midi_files(midi_files, seq_length)\n",
    "    logging.info(f\"Processed {len(training_data)} sequences from MIDI files.\")\n",
    "except ValueError as e:\n",
    "    logging.error(str(e))\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b6554a0-9c5a-4ecd-a1e6-39f04e5375a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader\n",
    "batch_size = 32\n",
    "dataset = TensorDataset(training_data)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85b2a893-b05b-4d46-b687-04b7eeed57bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Using MPS (GPU) for computation\n",
      "INFO:Using Apple Silicon GPU (MPS)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Composer model\n",
    "composer = Composer(load_trained=False)\n",
    "\n",
    "# Check if MPS (Apple Silicon GPU) is available\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    logging.info(\"Using Apple Silicon GPU (MPS)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    logging.info(\"Using CPU\")\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "composer = composer.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6de6fe09-5038-409c-8407-a438a01dd0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is using device: mps\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel is using device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mcomposer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PianoGen/composer_one.py:89\u001b[0m, in \u001b[0;36mComposer.train_model\u001b[0;34m(self, dataloader, num_epochs, learning_rate)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m     88\u001b[0m     batch \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)  \u001b[38;5;66;03m# Move input data to the same device as the model\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m     92\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloader)\n",
      "File \u001b[0;32m~/PianoGen/composer_one.py:100\u001b[0m, in \u001b[0;36mComposer.train_step\u001b[0;34m(self, batch, criterion, optimizer)\u001b[0m\n\u001b[1;32m     98\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, dim), batch[:, \u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))  \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[1;32m     99\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()  \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_grad_norm_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Gradient clipping\u001b[39;00m\n\u001b[1;32m    101\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# Update model parameters\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2022.10/lib/python3.9/site-packages/torch/nn/utils/clip_grad.py:82\u001b[0m, in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[0m\n\u001b[1;32m     80\u001b[0m         clip_coef_clamped_device \u001b[38;5;241m=\u001b[39m clip_coef_clamped\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m grads:\n\u001b[0;32m---> 82\u001b[0m             \u001b[43mg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclip_coef_clamped_device\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_norm\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set training parameters\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "print(f\"Model is using device: {device}\")\n",
    "\n",
    "# Train the model\n",
    "composer.train_model(data_loader, num_epochs, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6722f4b4-fbd8-4a2e-8ff3-80fef3bbdae4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
